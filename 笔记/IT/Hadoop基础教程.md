# Hadoop 基础教程

## 1 绪论

### 1.1 大数据处理

### 1.2 基于Amazon Web Services的云计算

### 1.3 小结

## 2 安装并运行Hadoop

### 2.1 基于本地Ubuntu主机的Hadoop系统

### 2.2 实践环节:检查是否已安装JDK

### 2.3 实践环节:下载Hadoop

### 2.4 实践环节:安装SSH

### 2.5 实践环节:使用Hadoop计算圆周率

### 2.6 实践环节:配置伪分布式模式

### 2.7 实践环节:修改HDFS根目录

### 2.8 实践环节:格式化NameNode

### 2.9 实践环节:启动Hadoop

### 2.10 实践环节:使用HDFS

### 2.11 实践环节:MapReduce的经典入门程序——字数统计

### 2.12 使用弹性MapReduce

### 2.13 实践环节:使用管理控制台在EMR运行

### 2.14 本地Hadoop与EMR Hadoop的对比

### 2.15 小结

## 3 理解MapReduce

### 3.1 键值对

### 3.2 MapReduce的Hadoop Java API

### 3.3 编写MapReduce程序

### 3.4 实践环节:设置classpath

### 3.5 实践环节:实现WordCount

### 3.6 实践环节:构建JAR文件

### 3.7 实践环节:在本地Hadoop集群运行WordCount

### 3.8 实践环节:在EMR上运行WordCount

### 3.9 实践环节:WordCount的简易运算

### 3.10 查看WordCount的运行全貌

### 3.11 实践环节:使用combiner编写WordCount

### 3.12 实践环节:更正使用combiner的WordCount

### 3.13 Hadoop专有数据类型

### 3.14 实践环节:使用Writeable包装类

### 3.15 输入/输出

### 3.16 小结

## 4 开发MapReduce程序

### 4.1 使用非Java语言操着Hadoop

### 4.2 实践环节:使用Streaming实现WordCount

### 4.3 分析大数据集

### 4.4 实践环节:统计汇总UFO

### 4.5 实践环节:统计形状数据

### 4.6 实践环节:找出目击事件的持续时间与UFO形状的关系

### 4.7 实践环节:在命令行中执行形状/时间分析

### 4.8 实践环节:使用ChainMapper进行字段验证/分析

### 4.9 实践环节:使用Distributed Cache改进地点输出

### 4.10 计数器、状态和其他输出

### 4.11 实践环节:创建计数器、任务状态和写入日志

### 4.12 小结

## 5 高级MapReduce技术

### 5.1 初级、高级还是中级

### 5.2 多数据源联结

### 5.3 实践环节:使用MultipleInputs实现reduce端联结

### 5.4 图算法

### 5.5 实践环节:图的表示

### 5.6 实践环节:创建源代码

### 5.7 实践环节:第一次运行作业

### 5.8 实践环节:第二次运行作业

### 5.9 实践环节:第三次运行作业

### 5.10 实践环节:第四次运行作业

### 5.11 使用语言无关的数据结构

### 5.12 实践环节:获取并安装Avro

### 5.13 实践环节:定义模式

### 5.14 实践环节:使用Ruby创建Avro数据源

### 5.15 实践环节:使用Java语言编程操作Avro数据

### 5.16 实践环节:在MapReduce中统计UFO形状

### 5.17 实践环节:使用Ruby检查输出数据

### 5.18 实践环节:使用Java检查输出数据

### 5.19 小结

## 6 故障处理

### 6.1 故障

### 6.2 实践环节:杀死DataNode进程

### 6.3 实践环节:复制因子的作用

### 6.4 实践环节:故意造成数据块丢失

### 6.5 实践环节:杀死TaskTracker进程

### 6.6 实践环节:杀死JobTracker

### 6.7 实践环节:杀死NameNode进程

### 6.8 实践环节:引发任务故障

### 6.9 数据原因造成的任务故障

### 6.10 实践环节:使用skip模式处理异常数据

### 6.11 小结

## 7 系统运行与维护

### 7.1 关于EMR的说明

### 7.2 Hadoop配置属性

### 7.3 实践环节:浏览默认属性

### 7.4 集群设置

### 7.5 实践环节:查看默认的机柜设置

### 7.6 实践环节:报告每台主机所在机柜

### 7.7 集群访问控制

### 7.8 实践环节:展示Hadoop的默认安全机制

### 7.9 管理NameNode

### 7.10 实践环节:为fsimage文件新增一个存储路径

### 7.11 实践环节:迁移到新的NameNode主机

### 7.12 管理HDFS

### 7.13 MapReduce的管理

### 7.14 实践环节:修改作业优先级并结束作业运行

### 7.15 扩展集群规模

### 7.16 小结

## 8 Hive:数据的关系视图

### 8.1 Hive概述

### 8.2 设置Hive

### 8.3 实践环节:安装Hive

### 8.4 使用Hive

### 8.5 实践环节:创建UFO数据表

### 8.6 实践环节:在表中插入数据

### 8.7 实践环节:验证表

### 8.8 实践环节:用正确的列分隔符重定义表

### 8.9 实践环节:基于现有文件创建表

### 8.10 实践环节:执行联结操作

### 8.11 实践环节:使用视图

### 8.12 实践环节:导出查询结果

### 8.13 实践环节:制作UFO目击事件分区表

### 8.14 实践环节:新增用户自定义函数

### 8.15 基于Amazon Web Services 的Hive

### 8.16 实践环节:在EMR上分析UFO数据

### 8.17 小结

## 9 与关系数据库协同工作

### 9.1 常见数据路径

### 9.2 配置MySQL

### 9.3 实践环节:安装并设置MySQL

### 9.4 实践环节:配置MySQL允许远程连接

### 9.5 实践环节:建立员工数据库

### 9.6 把数据导入Hadoop

### 9.7 实践环节:下载并配置Sqoop

### 9.8 实践环节:把MySQL数据导入HDFS

### 9.9 实践环节:把MySQL数据导出到Hive

### 9.10 实践环节:有选择性的导入数据

### 9.11 实践环节:使用数据类型映射

### 9.12 实践环节:通过原始查询导入数据

### 9.13 从Hadoop导出数据

### 9.14 实践环节:把Hadoop数据导入MySQL

### 9.15 实践环节:把Hive数据导入MySQL

### 9.16 实践环节:改进mapper并重新运行数据到处命令

### 9.17 在AWS上使用Sqoop

### 9.18 小结

## 10 使用Flume收集数据

### 10.1 关于AWS的说明

### 10.2 无处不在的数据

### 10.3 实践环节:吧网络服务器的数据导入Hadoop

### 10.4 Apache Flume简介

### 10.5 实践环节:安装并配置Flume

### 10.6 实践环节:把网络流量存入日志文件

### 10.7 实践环节:把日志输出到控制台

### 10.8 实践环节:把命令的执行结果写入平面文件

### 10.9 实践环节:把远程文件数据写入本地平面文件

### 10.10 实践环节:把网络数据写入HDFS

### 10.11 实践环节:加入时间戳

### 10.12 实践环节:多层Flume网络

### 10.13 实践环节:把实践写入多个信宿

### 10.14 更高的视角

### 10.15 小结

## 11 展望未来

### 11.1 全书回顾

### 11.2 即将到来的Hadoop变革

### 11.3 其他版本的Hadoop软件包

### 11.4 其他Apache项目
