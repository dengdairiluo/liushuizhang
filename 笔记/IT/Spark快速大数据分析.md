# Spark快速大数据分析

## 1 Spark数据分析导论

### 1.1 Spark是什么

### 1.2 一个大一统的软件栈

#### 1.2.1 Spark Core

#### 1.2.2 Spark SQL

#### 1.2.3 Spark Streaming

#### 1.2.4 MLlib

#### 1.2.5 GraphX

#### 1.2.6 集群管理器

### 1.3 Spark的用户和用途

#### 1.3.1 数据科学任务

#### 1.3.2 数据处理应用

### 1.4 Spark简史

### 1.5 Spark的版本和发布

### 1.6 Spark的存储层次

## 2 Spark下载与入门

### 2.1 下载Spark

### 2.2 SPark中python和Scala的shell

### 2.3 Spark核心概念简介

### 2.4 独立应用

#### 2.4.1 初始化SparkContext

#### 2.4.2 构建独立应用

### 2.5 总结

## 3 RDD编程

### 3.1 RDD基础

### 3.2 创建RDD

### 3.3 RDD操作

#### 3.3.1 转化操作

#### 3.3.2 行动操作

#### 3.3.3 惰性求值

### 3.4 向Spark传递函数

#### 3.4.1 Python

#### 3.4.2 Scala

#### 3.4.3 Java

### 3.5 常见的转化操作和行动操作

#### 3.5.1 基本的RDD

#### 3.5.2 在不同RDD类型间转换

### 3.6 持久化(缓存)

### 3.7 总结

## 4 键值对操作

### 4.1 动机

### 4.2 创建Pair RDD

### 4.3 Pair RDD的转化操作

#### 4.3.1 聚合操作

#### 4.3.2 数据分组

#### 4.3.3 连接

#### 4.3.4 数据排序

### 4.4 Pair RDD的行动操作

### 4.5 数据分区(进阶)

#### 4.5.1 获取RDD分区方式

#### 4.5.2 从分区中获益的操作

#### 4.5.3 影响分区方式的操作

#### 4.5.4 示例：PageRank

#### 4.5.5 自定义分区方式

### 4.6 总结

## 5 数据读取与保存

### 5.1 动机

### 5.2 文件格式

#### 5.2.1 文本文件

#### 5.2.2 JSON文件

#### 5.2.3 逗号分隔值与制表符分隔值

#### 5.2.4 SequenceFile

#### 5.2.5 对象文件

#### 5.2.6 Hadoop输入输出格式

#### 5.2.7 文件压缩

### 5.3 文件系统

#### 5.3.1 本地/“常规”文件系统

#### 5.3.2 Amazon S3

#### 5.3.3 HDFS

### 5.4 Spark SQL中的结构化

#### 5.4.1 Apache Hive

#### 5.4.2 JSON

### 5.5 数据库

#### 5.5.1 Java数据库连接

#### 5.5.2 Cassandra

#### 5.5.3 HBase

#### 5.5.4 Elasticsearch

### 5.6 总结

## 6 Spark编程进阶

### 6.1 简介

### 6.2 累加器

#### 6.2.1 累加器与容错性

#### 6.2.2 自定义累加器

### 6.3 广播变量

### 6.4 基于分区进行操作

### 6.5 与外部程序间的管道

### 6.6 数值RDD的操作

### 6.7 总结

## 7 在集群运行Spark

### 7.1 简介

### 7.2 Spark运行时架构

#### 7.2.1 驱动器节点

#### 7.2.2 执行器节点

#### 7.2.3 集群管理器

#### 7.2.4 启动一个程序

#### 7.2.5 小结

### 7.3 使用spark-submit部署应用

### 7.4 打包代码与依赖

#### 7.4.1 使用Maven构建的用Java编写的Spark应用

#### 7.4.2 使用sbt构建的用Scala编写的Spark应用

#### 7.4.3 依赖冲突

### 7.5 Spark应用内鱼应用间调度

### 7.6 集群管理器

#### 7.6.1 独立集群管理器

#### 7.6.2 Hadoop YARN

#### 7.6.3 Apache Mesos

#### 7.6.4 Amazon EC2

### 7.7 选择合适的集群管理器

### 7.8 总结

## 8 Spark调优与调试

### 8.1 使用SparkConf配置Spark

### 8.2 Spark执行的组成部分：作业、任务和步骤

### 8.3 查找信息

#### 8.3.1 Spark网页用户页面

### 8.4 关键性能考量

#### 8.4.1 并行度

#### 8.4.2 序列化格式

#### 8.4.3 内存管理

#### 8.4.4 硬件供给

### 8.5 总结

## 9 Spark SQL

### 9.1 连接Spark SQL

### 9.2 在应用中使用Spark SQL

#### 9.2.1 初始化Spark SQL

#### 9.2.2 基本查询示例

#### 9.2.3 SchemaRDD

#### 9.2.4 缓存

### 9.3 读取和存储数据

#### 9.3.1 Apache Hive

#### 9.3.2 Parquet

#### 9.3.3 JSON

#### 9.3.4 基于RDD

### 9.4 JDBC/ODBC服务器

#### 9.4.1 使用Beeline

#### 9.4.2 长生命周期的表与查询

### 9.5 用户自定义函数

#### 9.5.1 Spark SQL UDF

#### 9.5.2  Hive UDF

### 9.6 Spark SQL 性能

### 9.7 总结

## 10 Spark Streaming

### 10.1 一个简单的例子

### 10.2 架构与抽象

### 10.3 转化操作

#### 10.3.1 无状态转化操作

#### 10.3.2 有状态转化操作

### 10.4 输出操作

### 10.5 输入源

#### 10.5.1 核心数据源

#### 10.5.2 附加数据源

#### 10.5.3 多数据源于集群规模

### 10.6 24/7不间断运行

#### 10.6.1 检查点机制

#### 10.6.2 驱动器程序容错

#### 10.6.3 工作节点容错

#### 10.6.4 接收器容错

#### 10.6.5 处理保证

### 10.7 Streaming用户界面

### 10.8 性能考量

#### 10.8.1 批次和窗口大小

#### 10.8.2 并行度

#### 10.8.3 垃圾回收和内存使用

### 10.9 总结

## 11 基于MLlib的机器学习

### 11.1 概述

### 11.2 系统要求

### 11.3 机器学习基础

### 11.4 数据类型

### 11.5 算法

#### 11.5.1 特征提取

#### 11.5.2 统计

#### 11.5.3 分类与回归

#### 11.5.4 聚类

#### 11.5.5 协同过滤与推荐

#### 11.5.6 降维

#### 11.5.7 模型评估

### 11.6 一些提示与性能考量

#### 11.6.1 准备特征

#### 11.6.2 配置算法

#### 11.6.3 缓存RDD以重复使用

#### 11.6.4 识别稀疏程度

#### 11.6.5 并行度

### 11.7 流水线API

### 11.8 总结
