# Spark快速大数据分析

## 1 Spark数据分析导论

### 1.1 Spark是什么

### 1.2 一个大一统的软件栈

#### 1.2.1 Spark Core

#### 1.2.2 Spark SQL

#### 1.2.3 Spark Streaming

#### 1.2.4 MLlib

#### 1.2.5 GraphX

#### 1.2.6 集群管理器

### 1.3 Spark的用户和用途

#### 1.3.1 数据科学任务

#### 1.3.2 数据处理应用

### 1.4 Spark简史

### 1.5 Spark的版本和发布

### 1.6 Spark的存储层次

## 2 Spark下载与入门

### 2.1 下载Spark

### 2.2 SPark中python和Scala的shell

### 2.3 Spark核心概念简介

### 2.4 独立应用

#### 2.4.1 初始化SparkContext

#### 2.4.2 构建独立应用

### 2.5 总结

## 3 RDD编程

### 3.1 RDD基础

### 3.2 创建RDD

### 3.3 RDD操作

#### 3.3.1 转化操作

#### 3.3.2 行动操作

#### 3.3.3 惰性求值

### 3.4 向Spark传递函数

#### 3.4.1 Python

#### 3.4.2 Scala

#### 3.4.3 Java

### 3.5 常见的转化操作和行动操作

#### 3.5.1 基本的RDD

#### 3.5.2 在不同RDD类型间转换

### 3.6 持久化(缓存)

### 3.7 总结

## 4 键值对操作

### 4.1 动机

### 4.2 创建Pair RDD

### 4.3 Pair RDD的转化操作

#### 4.3.1 聚合操作

#### 4.3.2 数据分组

#### 4.3.3 连接

#### 4.3.4 数据排序

### 4.4 Pair RDD的行动操作

### 4.5 数据分区(进阶)

#### 4.5.1 获取RDD分区方式

#### 4.5.2 从分区中获益的操作

#### 4.5.3 影响分区方式的操作

#### 4.5.4 示例：PageRank

#### 4.5.5 自定义分区方式

### 4.6 总结

## 5 数据读取与保存

### 5.1 动机

### 5.2 文件格式

#### 5.2.1 文本文件

#### 5.2.2 JSON文件

#### 5.2.3 逗号分隔值与制表符分隔值

#### 5.2.4 SequenceFile

#### 5.2.5 对象文件

#### 5.2.6 Hadoop输入输出格式

#### 5.2.7 文件压缩

### 5.3 文件系统

#### 5.3.1 本地/“常规”文件系统

#### 5.3.2 Amazon S3

#### 5.3.3 HDFS

### 5.4 Spark SQL中的结构化

#### 5.4.1 Apache Hive

#### 5.4.2 JSON

### 5.5 数据库

#### 5.5.1 Java数据库连接

#### 5.5.2 Cassandra

#### 5.5.3 HBase

#### 5.5.4 Elasticsearch

### 5.6 总结

## 6 Spark编程进阶

### 6.1 简介

### 6.2 累加器

#### 6.2.1 累加器与容错性

#### 6.2.2 自定义累加器

### 6.3 广播变量

### 6.4 基于分区进行操作

### 6.5 与外部程序间的管道

### 6.6 数值RDD的操作

### 6.7 总结

## 7 在集群运行Spark

### 7.1 简介

### 7.2 Spark运行时架构

#### 7.2.1 驱动器节点

#### 7.2.2 执行器节点

#### 7.2.3 集群管理器

#### 7.2.4 启动一个程序

#### 7.2.5 小结

### 7.3 使用spark-submit部署应用
